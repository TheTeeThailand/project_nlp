{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f85e73-93b0-45cf-b737-af8758631ca7",
   "metadata": {},
   "source": [
    "# Phase 0: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dea75e1f-99af-4d48-bc74-250e8bd0806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Library Versions\n",
      "------------------------------\n",
      "Python version      : 3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]\n",
      "NumPy version       : 1.23.5\n",
      "Pandas version      : 2.3.1\n",
      "Scikit-learn version: 1.7.1\n",
      "Matplotlib version  : 3.10.5\n",
      "Seaborn version     : 0.13.2\n",
      "Transformers version: 4.30.2\n",
      "Huggingface Datasets: 4.0.0\n",
      "PyTorch version     : 2.7.1\n",
      "TQDM version        : 4.67.1\n",
      "CPU architecture    : arm\n"
     ]
    }
   ],
   "source": [
    "# System Info\n",
    "import sys         # Access system-specific parameters and functions\n",
    "import os          # Interface with the operating system\n",
    "import platform    # Retrieve underlying platform and hardware information\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np         # Numerical operations and arrays\n",
    "import pandas as pd        # Data manipulation with DataFrames\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns            # Statistical data visualization\n",
    "\n",
    "# Machine Learning & NLP\n",
    "import sklearn              # Scikit-learn for ML tools (baseline models, metrics)\n",
    "import transformers         # Hugging Face Transformers (e.g., BERT)\n",
    "import datasets             # Hugging Face Datasets for loading NLP corpora\n",
    "\n",
    "# Deep Learning\n",
    "import torch                # PyTorch for tensor ops and training\n",
    "\n",
    "# Progress Bars\n",
    "import tqdm             # Progress bars for loops and training\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Check versions\n",
    "print(\"✅ Library Versions\\n\" + \"-\"*30)\n",
    "print(f\"Python version      : {sys.version}\")\n",
    "print(f\"NumPy version       : {np.__version__}\")\n",
    "print(f\"Pandas version      : {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Matplotlib version  : {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version     : {sns.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Huggingface Datasets: {datasets.__version__}\")\n",
    "print(f\"PyTorch version     : {torch.__version__}\")\n",
    "print(f\"TQDM version        : {tqdm.__version__}\")\n",
    "print(f\"CPU architecture    : {platform.processor() or platform.machine()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "670009bb-a291-466d-9f1c-078926b917c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34272 examples\n",
      "{'text': \"Today, many adults or teenage drivers are hooked onto their phones. While driving, they can be prompted to use their phones for text messaging. It may cause many accidents, death, serious injuries and more. I honestly think that drivers should use phones while driving  because they are taking risks that could kill or injure others and also yourself. There are also laws against using a phone while operating a moving vehicle but people still disobey them. I think that there should be more consequences when it comes down to texting and driving.  Using your cell phones causes many distractions. It only takes a blink of an eye to cause an accident. Yeah, resisting the urge to text while driving may be hard but it can and will save lives including yours. When driving, your eyes and mind are programmed to be focused on the road at all times. Having a cellphone on your person is a hazard in my opinion. Just think about it, anyone could be crossing a busy street. The person walking is probably expecting you to slow down and obey the street laws. Meanwhile, you're responding to a text that could have waited, now you hit that person and he lost his life. All because of your cell phone. If there is a law for it then you follow that law because there is a reason for it being in full effect. Just imagine you lost a close friend or family member due to breaking the same law you once broke. The death toll in Georgia of people losing their lives to texting and driving drivers is 3,166. That number has increased dramatically ever since 2017. We could do so much better and help decrease that number to at least zero to little deaths. That is unacceptable, literally no one wants to have news broken down to them on how their mom, sister, brother, etc has died in a texting and driving incident.  People have their phones on every day. They act as if they cannot live without their cell phone. I get you need your phone for business, social media, long distance calling or texting but that can wait. People should take in consideration of others and just make the roads safer than what they should be. No one is so busy that they need to hurry and send a last minute text or answer a last minute phone call. I believe that everyone has patience and that they should use it.  We have come to a society where phones are more important than our lives itself. That should never be the case, if anything I personally think cellphones should be banned while in a motorized vehicle. Choosing to value a text message over your own life is showing us you have no respect for yourself or for others. They should be more considerate and put phones down and make sure they are paying more attention to the road.         \", 'label': 0, 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to SemEval2024 Task 8 monolingual data\n",
    "data_path = \"../data/semeval/subtaskA_monolingual.jsonl\"\n",
    "\n",
    "# Load data\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Preview first example\n",
    "print(f\"Loaded {len(data)} examples\")\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450060d5-32a2-467a-a9ea-2edc7de269ec",
   "metadata": {},
   "source": [
    "# Phase 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "090c3adb-e222-4c15-b361-6518fe9594a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today, many adults or teenage drivers are hook...</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The automobile, since its advent, has revoluti...</td>\n",
       "      <td>1</td>\n",
       "      <td>refute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One policy that could potentially improve aca...</td>\n",
       "      <td>1</td>\n",
       "      <td>refute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: Navigating the Road Ahead: The Case for...</td>\n",
       "      <td>1</td>\n",
       "      <td>refute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have you ever woken up in the morning and wish...</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_text\n",
       "0  Today, many adults or teenage drivers are hook...      0    support\n",
       "1  The automobile, since its advent, has revoluti...      1     refute\n",
       "2   One policy that could potentially improve aca...      1     refute\n",
       "3  Title: Navigating the Road Ahead: The Case for...      1     refute\n",
       "4  Have you ever woken up in the morning and wish...      0    support"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert JSON list to DataFrame first\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label mapping (from integers to string labels)\n",
    "label_map = {\n",
    "    0: \"support\",\n",
    "    1: \"refute\",\n",
    "    2: \"no_relation\"\n",
    "}\n",
    "\n",
    "# Create new column with text labels\n",
    "df[\"label_text\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "# Preview\n",
    "df[[\"text\", \"label\", \"label_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "237505cb-5bbf-46f0-95b0-2b869b61e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 27417, Test size: 6855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 1    0.525222\n",
       " 0    0.474778\n",
       " Name: proportion, dtype: float64,\n",
       " label\n",
       " 1    0.525164\n",
       " 0    0.474836\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data (stratify for balanced labels)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "train_df[\"label\"].value_counts(normalize=True), test_df[\"label\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded9987-e1ef-4b1e-87a6-237ad6fb16a5",
   "metadata": {},
   "source": [
    "# Phase 2: Tokenization + BERT Input Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "215dae35-9ff9-4a46-bee6-54e2cf4f9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize train and test texts\n",
    "train_encodings = tokenizer(list(train_df[\"text\"]), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_df[\"text\"]), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels\n",
    "import torch\n",
    "train_labels = torch.tensor(train_df[\"label\"].values)\n",
    "test_labels = torch.tensor(test_df[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f363e4fe-fe65-4f94-aa12-de1d5fdb086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Train shape: torch.Size([27417, 128])\n",
      "📝 Sample decode: [CLS] it would be make out community better if the principle would make everyone do community service. people should be required to do it because it would make our environment cleaner. people would take more respect in our community. people would have a responsibility to clean up after themselves and others. most people don't think about our community before they do something wrong to it. it needs to be kept clean and not have trash everywhere. since they are the people to clean it they wouldn't destroy our community. also many people grafiti on things. things are ruined from spray paint and markings all over walls or buildings. people need to clean our [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\"🔢 Train shape:\", train_encodings[\"input_ids\"].shape)\n",
    "print(\"📝 Sample decode:\", tokenizer.decode(train_encodings[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed451987-f4e3-4652-8899-edac2c6714e2",
   "metadata": {},
   "source": [
    "# Phase 3: Train BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae6f9f8b-3692-433f-b377-5f5a57f580f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # dict with input_ids, attention_mask\n",
    "        self.labels = labels        # list or tensor of labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each item is a dict of {input_ids, attention_mask, label}\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03a7d9a0-7861-4536-81db-13375288f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize train/test\n",
    "train_encodings = tokenizer(list(train_df[\"text\"]), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(list(test_df[\"text\"]), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Prepare labels\n",
    "train_labels = list(train_df[\"label\"])\n",
    "test_labels = list(test_df[\"label\"])\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = TextClassificationDataset(train_encodings, train_labels)\n",
    "test_dataset = TextClassificationDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ef628f3-32fd-412b-88b5-512c380a2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Sample from train_dataset[0]:\n",
      "input_ids: torch.Size([256]) → tensor([  101,  2009,  2052,  2022,  2191,  2041,  2451,  2488,  2065,  1996,\n",
      "         6958,  2052,  2191,  3071,  2079,  2451,  2326,  1012,  2111,  2323,\n",
      "         2022,  3223,  2000,  2079,  2009,  2138,  2009,  2052,  2191,  2256,\n",
      "         4044, 20133,  1012,  2111,  2052,  2202,  2062,  4847,  1999,  2256,\n",
      "         2451,  1012,  2111,  2052,  2031,  1037,  5368,  2000,  4550,  2039,\n",
      "         2044,  3209,  1998,  2500,  1012,  2087,  2111,  2123,  1005,  1056,\n",
      "         2228,  2055,  2256,  2451,  2077,  2027,  2079,  2242,  3308,  2000,\n",
      "         2009,  1012,  2009,  3791,  2000,  2022,  2921,  4550,  1998,  2025,\n",
      "         2031, 11669,  7249,  1012,  2144,  2027,  2024,  1996,  2111,  2000,\n",
      "         4550,  2009,  2027,  2876,  1005,  1056,  6033,  2256,  2451,  1012,\n",
      "         2036,  2116,  2111, 22160, 25090,  2006,  2477,  1012,  2477,  2024,\n",
      "         9868,  2013, 12509,  6773,  1998, 13967,  2035,  2058,  3681,  2030,\n",
      "         3121,  1012,  2111,  2342,  2000,  4550,  2256,  4044,  2061,  2009,\n",
      "         2064,  2197,  2936,  1998,  2022,  2172, 20133,  2005,  2149,  2035,\n",
      "         1012,  2116,  2477,  2024,  2074,  2893,  2589,  1998,  2053,  2028,\n",
      "         2003,  2383,  4847,  2000,  1996,  2111,  2008,  2941,  2175,  2041,\n",
      "         2045,  1998,  4550,  2009,  2035,  2039,  1998,  3046,  1012,  2065,\n",
      "         2062,  2111,  2052,  3046,  2000,  2025, 19070,  1998,  2079,  2060,\n",
      "         2477,  2000,  2256,  2451,  2084,  2009,  2876,  1005,  1056,  2022,\n",
      "         2431,  2004,  2919,  2004,  2009,  2003,  1012,  2111,  2123,  1005,\n",
      "         1056,  2113,  2054,  2009,  2003,  2725,  2000,  2256,  4044,  1012,\n",
      "         2061,  2008,  2027,  2079,  2113,  2055,  2009,  2062,  2027,  2323,\n",
      "         2022,  2041,  2045,  9344,  2009,  1998,  3773,  2129,  4470,  2319,\n",
      "         2256,  2451,  2003,  1012,  2087,  2111,  2123,  1005,  1056,  2031,\n",
      "         2000,  2079, 27091,  2012,  2188,  1012,  2023,  2052,  2507,  2068,\n",
      "         1037,  2126,  2000,  4553,  2129,   102])\n",
      "token_type_ids: torch.Size([256]) → tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "attention_mask: torch.Size([256]) → tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels: torch.Size([]) → 0\n"
     ]
    }
   ],
   "source": [
    "# Preview first training example\n",
    "sample = train_dataset[0]\n",
    "print(\"🔍 Sample from train_dataset[0]:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {value.shape} → {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59a90b5d-eb37-41c2-b30d-6c0d6147efbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it would be make out community better if the principle would make everyone do community service. people should be required to do it because it would make our environment cleaner. people would take more respect in our community. people would have a responsibility to clean up after themselves and others. most people don't think about our community before they do something wrong to it. it needs to be kept clean and not have trash everywhere. since they are the people to clean it they wouldn't destroy our community. also many people grafiti on things. things are ruined from spray paint and markings all over walls or buildings. people need to clean our environment so it can last longer and be much cleaner for us all. many things are just getting done and no one is having respect to the people that actually go out there and clean it all up and try. if more people would try to not litter and do other things to our community than it wouldn't be half as bad as it is. people don't know what it is doing to our environment. so that they do know about it more they should be out there cleaning it and seeing how unclean our community is. most people don't have to do chores at home. this would give them a way to learn how\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97400c43-1a38-4c69-ac1f-73eadf92aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "91eb2979-9b48-4621-ba54-ccf92b7b87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 A Batch from train_loader\n",
      "input_ids shape     : torch.Size([16, 256])\n",
      "attention_mask shape: torch.Size([16, 256])\n",
      "labels shape        : torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Preview one batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"📦 A Batch from train_loader\")\n",
    "print(f\"input_ids shape     : {batch['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"labels shape        : {batch['labels'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02416514-8d47-4ef7-acaf-1901c25f07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler (optional but helpful for training stability)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9aa1878f-ffc7-4e5a-99a9-230e788b09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c3aab-24da-439d-9952-b582be731913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 🔧 Disable tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set number of epochs (run 3 times)\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n Epoch {epoch+1}/{num_epochs}\")\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        # Move batch to device (CPU or MPS)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Remove token_type_ids if present (DistilBERT doesn't use them)\n",
    "        batch.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Log loss\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Save model weights after training (PyTorch format)\n",
    "torch.save(model.state_dict(), \"distilbert_trained_model.pt\")\n",
    "print(\" Model saved to distilbert_trained_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742532e-5edb-42c3-8828-d83e22a2f052",
   "metadata": {},
   "source": [
    "# Phase 4: Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52878dc8-ed0e-46f3-b8cc-2f0f07124fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# No gradient needed\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch.pop(\"token_type_ids\", None)  # Remove if exists\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bf69c-37c9-4805-bea9-84c7aad7475d",
   "metadata": {},
   "source": [
    "### Evaluate with Accuracy, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fffde-1ea0-4015-adaa-6b48918966dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore undefined metric warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "acc_percent = acc * 100\n",
    "\n",
    "# Classification Report as Dict\n",
    "report_dict = classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=[0, 1],\n",
    "    target_names=[\"Support\", \"Refute\"],\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(report_dict).T\n",
    "\n",
    "# Select relevant rows\n",
    "df_clean = df.loc[[\"Support\", \"Refute\", \"macro avg\", \"weighted avg\"], [\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
    "\n",
    "# Rename index\n",
    "df_clean.index = [\"Support\", \"Refute\", \"Macro Avg\", \"Weighted Avg\"]\n",
    "df_clean.columns = [\"Precision\", \"Recall\", \"F1-score\", \"Support (samples)\"]\n",
    "\n",
    "# Round scores\n",
    "df_clean = df_clean.round(4)\n",
    "\n",
    "# Display table\n",
    "print(\"📊 Classification Report Summary (DistilBERT - Test Set)\")\n",
    "display(df_clean)\n",
    "\n",
    "# Print Accuracy\n",
    "print(f\"\\n Overall Accuracy: {acc:.4f} ({acc_percent:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_nlp",
   "language": "python",
   "name": "project_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
